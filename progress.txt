# Media Concierge Bot - Progress Log

This file tracks learnings, decisions, and blockers across Ralph iterations.
Each entry should be timestamped and reference the story ID.

---

## Session Start

Project initialized. Ready for first iteration.

### Decisions Made
- Using python-telegram-bot 21.x (async)
- Using Claude claude-sonnet-4-5-20250929 for tool_use
- SQLite for user storage (simple, file-based)
- Pydantic for all data models

### Known Challenges
- Rutracker may require proxy/VPN from some regions
- Trakt tokens expire every 24 hours - need auto-refresh
- sendMessageDraft is new Telegram API - verify support in python-telegram-bot

---

## Iteration Log

### Iteration 1 - INFRA-001: Project Structure Setup
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Created `pyproject.toml` with all required dependencies:
  - python-telegram-bot 21.x for async Telegram bot
  - anthropic 0.40+ for Claude API
  - httpx, pydantic, structlog, aiosqlite, cryptography, beautifulsoup4, lxml
  - Dev dependencies: pytest, pytest-asyncio, pytest-cov, ruff, mypy
- Created multi-stage `Dockerfile` optimized for production:
  - Builder stage with compilation dependencies
  - Runtime stage with minimal image size (target < 500MB)
  - Health check endpoint configured
  - Non-root user for security
  - Supports webhook mode for Koyeb deployment
- Verified all acceptance criteria:
  - ✅ Directory structure exists (src/, tests/, data/, docs/)
  - ✅ pyproject.toml with all dependencies
  - ✅ .env.example with all environment variables
  - ✅ .gitignore for Python project
  - ✅ Dockerfile for Koyeb
  - ✅ README.md with project description

#### Learnings:
- Project structure was already partially set up, needed pyproject.toml and Dockerfile
- Using ruff for linting/formatting (modern, fast alternative to black + flake8)
- Dockerfile uses multi-stage build to minimize final image size
- All async dependencies selected for python-telegram-bot 21.x compatibility

#### Next steps:
- INFRA-002: Configuration Management (src/config.py with pydantic-settings)

---

### Iteration 2 - INFRA-002: Configuration Management
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Created `src/config.py` with pydantic-settings for configuration management
- Implemented Settings class with all required and optional environment variables:
  - Required: TELEGRAM_BOT_TOKEN, ANTHROPIC_API_KEY, TMDB_API_KEY, KINOPOISK_API_TOKEN, ENCRYPTION_KEY
  - Optional: SEEDBOX_HOST, SEEDBOX_USER, SEEDBOX_PASSWORD
  - App config: LOG_LEVEL, ENVIRONMENT, CACHE_TTL, WEBHOOK_URL, WEBHOOK_PATH, PORT
- All sensitive fields use `SecretStr` type to prevent accidental logging
- Added field validators for log_level and environment
- Implemented helper properties: is_development, is_production, has_seedbox
- Created `get_safe_dict()` method that masks sensitive values with '***'
- Verified all acceptance criteria:
  - ✅ src/config.py created with pydantic-settings
  - ✅ All env variables properly typed (SecretStr for sensitive data, str/int for others)
  - ✅ Required variables validated (raises ValidationError if missing)
  - ✅ Sensitive data not logged (SecretStr + get_safe_dict() for safe logging)

#### Learnings:
- Using SecretStr prevents accidental logging of secrets in exceptions and logs
- pydantic-settings automatically loads from .env file with env_file setting
- Field validators can enforce allowed values (e.g., log levels, environments)
- Property methods provide convenient helpers for common checks
- get_safe_dict() is useful for logging configuration without exposing secrets
- ruff formatter adjusts list formatting in properties

#### Verification:
- ✓ `python -c 'from src.config import settings'` works without errors
- ✓ Validation error raised when required variables missing
- ✓ Sensitive values properly masked in get_safe_dict()
- ✓ Can access secret values via get_secret_value() when needed
- ✓ Linter and formatter pass without issues

#### Next steps:
- BOT-001: Basic Telegram Bot (src/bot/main.py with handlers)

---

### Iteration 3 - INFRA-003: Logging Setup
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Created `src/logger.py` with structlog configuration
- Implemented environment-aware logging:
  - **Production mode (Koyeb)**: JSON-formatted logs for log aggregation
  - **Development mode**: Colored console output with human-friendly formatting
- Configured structured logging processors:
  - Automatic timestamp injection (ISO format)
  - Log level normalization (warn → warning)
  - Stack trace and exception formatting
  - Sensitive data censoring processor
- Implemented `censor_sensitive_data()` processor that automatically masks:
  - Tokens, passwords, API keys, secrets, authorization headers
  - Encryption keys, credentials, session data, cookies
  - Works recursively on nested dicts and lists
- Created `get_logger()` helper function for easy logger instantiation
- Verified all acceptance criteria:
  - ✅ src/logger.py created with structlog configuration
  - ✅ JSON format output in production (ENVIRONMENT=production)
  - ✅ Console-friendly colored output in development
  - ✅ Different log levels configurable via LOG_LEVEL env var
  - ✅ Sensitive data automatically censored with "***" in logs

#### Learnings:
- **Structlog processors**: Chain of responsibility pattern for transforming log events
  - Processors run in order, each can modify the event dict
  - Final renderer converts event dict to output format (JSON or Console)
- **Type hints with structlog**: EventDict is MutableMapping[str, Any], not dict
  - Need to be careful with type annotations to avoid mypy/pyright errors
- **Sensitive data detection**: Pattern matching on key names (case-insensitive)
  - Keywords like "token", "password", "api_key" trigger censoring
  - Applied recursively to handle nested structures
- **Environment-based configuration**: Using settings.is_production for renderer selection
  - Production: JSONRenderer for machine-readable logs
  - Development: ConsoleRenderer with colors for human readability
- **Stdlib integration**: structlog integrates with Python's logging module
  - ProcessorFormatter bridges structlog processors with stdlib logging
  - Allows gradual adoption in projects using stdlib logging
- **Linter compliance**: ruff prefers early returns over elif chains (RET505)
  - Replaced `elif` with separate `if` statements after returns

#### Verification:
- ✓ Test 1: Logger import works without errors
- ✓ Test 2: Production mode outputs valid JSON format
  - Example: `{"user_id": 123, "status": "success", "event": "test_message", "logger": "__main__", "level": "info", "timestamp": "2026-01-18T13:25:44.081111Z"}`
- ✓ Test 3: Sensitive data automatically censored
  - `token`, `api_key`, `password`, `encryption_key` all replaced with "***"
- ✓ Test 4: Development mode shows colored console output
  - Includes timestamp, level (colored), event name, logger name, and context fields
- ✓ Linter passes: `ruff check . --fix` and `ruff format .` both pass

#### Next steps:
- BOT-001: Basic Telegram Bot (src/bot/main.py with /start and /help handlers)

---

### Iteration 4 - BOT-001: Basic Telegram Bot
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Created `src/bot/__init__.py` as module entry point
- Created `src/bot/handlers.py` with command handlers:
  - `start_handler()`: Welcomes user with feature overview and markdown formatting
  - `help_handler()`: Shows command list and usage examples
  - `error_handler()`: Global error handler with structured logging
- Created `src/bot/main.py` with full bot initialization:
  - `create_application()`: Sets up Telegram bot with handlers
  - `run_polling()`: Development mode with polling
  - `run_webhook()`: Production mode with webhook for Koyeb
  - `main()`: Entry point with proper async handling
- Verified all acceptance criteria:
  - ✅ src/bot/main.py created with bot initialization
  - ✅ /start handler with welcoming message in Russian
  - ✅ /help handler with command list and examples
  - ✅ Bot starts without errors (tested with create_application())
  - ✅ Webhook mode for production (auto-detected from settings)

#### Learnings:
- **python-telegram-bot 21.x architecture**:
  - Fully async with Application.builder() pattern
  - Separate updater for polling vs webhook modes
  - CommandHandler for /commands, MessageHandler for text messages
  - Error handlers receive (update, context) with error in context.error
- **Webhook vs Polling modes**:
  - Polling: For development, uses updater.start_polling()
  - Webhook: For production (Koyeb), uses updater.start_webhook() on 0.0.0.0
  - Webhook requires set_webhook() to register URL with Telegram
  - Settings determine mode: is_production and webhook_url presence
- **Handler structure**:
  - Each handler is async and receives (Update, ContextTypes.DEFAULT_TYPE)
  - Use update.effective_user for user info
  - Use update.message.reply_text() for responses
  - Markdown formatting works but needs fallback for errors
- **Error handling pattern**:
  - Try/except in each handler with fallback plain text
  - Global error_handler catches uncaught exceptions
  - Structured logging with context (user_id, error details)
- **Linter compliance**:
  - ruff auto-fixed import order (removed unused structlog import)
  - ruff formatted long builder chain to single line
  - __all__ in __init__.py requires actual imports to avoid warnings

#### Verification:
- ✓ `python -c 'from src.bot.main import create_application; create_application()'` works
- ✓ Application created with 3 handlers (/start, /help, /health)
- ✓ Error handler registered successfully
- ✓ Webhook mode enabled for production environment
- ✓ All imports resolve correctly
- ✓ Linter passes without errors

#### Next steps:
- BOT-002: Message Streaming with sendMessageDraft

---

### Iteration 5 - BOT-002: Message Streaming with sendMessageDraft
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Created `src/bot/streaming.py` with comprehensive streaming functionality:
  - `StreamingMessageHandler` class for managing progressive message updates
  - Automatic typing indicator that re-sends every 4 seconds (Telegram requirement)
  - Rate-limited message updates (configurable interval, default 0.5s)
  - Progressive text accumulation from async iterators
  - Graceful error handling with message preservation
- Implemented key features:
  - `start_typing()` / `stop_typing()`: Manage typing indicator lifecycle
  - `send_initial_message()`: Send placeholder message to be updated
  - `update_message()`: Update message with rate limiting and error handling
  - `finalize_message()`: Force-send final complete message
  - `stream_text()`: Main entry point for streaming from async iterator
  - `send_streaming_message()`: Convenience function for one-liner usage
- Created comprehensive test suite in `tests/test_streaming.py`:
  - 9 unit tests covering all major functionality
  - Tests for typing indicator, rate limiting, error handling, streaming
  - All tests passing with proper async/await patterns
  - Mock Telegram objects for isolated testing
- Updated `src/bot/__init__.py` to export streaming module
- Verified all acceptance criteria:
  - ✅ src/bot/streaming.py created with streaming logic
  - ✅ Messages update progressively as content is generated
  - ✅ Error handling ensures message is sent even if streaming fails
  - ✅ Typing indicator shows during generation (auto-renewed every 4s)

#### Learnings:
- **Telegram typing indicator behavior**:
  - ChatAction.TYPING expires after 5 seconds
  - Must re-send every 4-5 seconds to maintain indicator
  - Implemented background asyncio task that loops until stopped
  - Used contextlib.suppress() for clean CancelledError handling (ruff suggestion)
- **Message streaming pattern**:
  - No "sendMessageDraft" API in python-telegram-bot or Telegram Bot API
  - Standard approach: send initial message, then use `edit_message_text()` to update
  - Rate limiting essential to avoid Telegram API limits (429 Too Many Requests)
  - Configurable update interval (default 0.5s) balances UX and API limits
- **Error handling strategy**:
  - BadRequest errors: Check for "not modified" (text unchanged)
  - RetryAfter errors: Wait specified time and retry
  - TimedOut errors: Log but don't fail (message will finalize anyway)
  - On any error: Always try to send accumulated text to user
- **Type safety with Telegram objects**:
  - update.effective_chat and update.message can be None in edge cases
  - Added null checks before accessing properties to satisfy Pyright
  - Used `raise ValueError()` for programmer errors vs graceful degradation for runtime errors
- **Testing async generators**:
  - Need to define generators inside test functions for proper async context
  - AsyncMock works well for mocking Telegram API calls
  - MagicMock with spec= provides good type checking in tests
- **Code quality**:
  - ruff auto-fixed import order (collections.abc.AsyncIterator preferred over typing.AsyncIterator)
  - contextlib.suppress() cleaner than try/except/pass for expected exceptions
  - Comprehensive docstrings in Google style format
  - All public methods documented with Args/Returns/Raises

#### Architecture decisions:
- **Rate limiting approach**: Time-based with configurable interval
  - Alternative: Token bucket algorithm (more complex, not needed yet)
  - Alternative: Fixed batch size (less smooth UX)
- **Typing indicator**: Background task pattern
  - Alternative: Manual calls (error-prone, easy to forget)
  - Alternative: Context manager (doesn't handle long-running operations well)
- **Error recovery**: Best-effort message delivery
  - Always try to send what was accumulated
  - Re-raise error after sending (caller can decide how to handle)
  - Log all errors with structured context

#### Verification:
- ✓ All 9 unit tests pass
- ✓ Streaming handler creates messages and updates them progressively
- ✓ Typing indicator starts/stops correctly
- ✓ Rate limiting prevents excessive API calls
- ✓ Error handling preserves accumulated text
- ✓ Convenience function works end-to-end
- ✓ Linter passes: `ruff check . --fix && ruff format .`
- ✓ Type checker passes (only acceptable warnings in error recovery code)

#### Integration notes:
- **Next steps for integration**:
  - AI-001 will use this streaming module with Claude API
  - Claude's streaming API yields text chunks: `async for chunk in stream`
  - Call `send_streaming_message(update, context, claude_stream())`
  - Typing indicator will show automatically during generation
- **Configuration options**:
  - `update_interval`: Control update frequency (default 0.5s)
  - `min_update_length`: Minimum chars before first update (default 20)
  - `initial_text`: Placeholder while generating (default "...")

#### Next steps:
- AI-001: Claude API Integration (will use streaming for responses)

---

### Iteration 6 - AI-001: Claude API Integration
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Created `src/ai/__init__.py` as module entry point with exports
- Created `src/ai/prompts.py` with system prompts:
  - `MEDIA_CONCIERGE_SYSTEM_PROMPT`: Comprehensive Russian-language prompt for media assistant
  - `get_system_prompt()`: Function to append user preferences to base prompt
  - Supports quality, language, and genre preferences injection
- Created `src/ai/claude_client.py` with full Claude API integration:
  - `Message`, `ToolCall`, `ToolResult` dataclasses for clean data handling
  - `ConversationContext` class for managing conversation history with trimming
  - `ClaudeClient` class with async client for Anthropic API
  - Model: claude-sonnet-4-5-20250929 (as specified in prd.json)
- Implemented streaming support:
  - `stream_message()`: Async iterator yielding text chunks as generated
  - Handles streaming events: content_block_start, content_block_delta, content_block_stop
  - Accumulates tool call JSON during streaming for later execution
- Implemented tool_use mechanism:
  - `_execute_tool()`: Executes tool calls via configurable executor function
  - `_process_response()`: Handles multi-turn tool conversations (up to 10 iterations)
  - Proper error handling for tool execution failures
  - Tool results sent back to Claude for continued conversation
- Created comprehensive test suite in `tests/test_claude_client.py`:
  - 21 unit tests covering all major functionality
  - Tests for dataclasses, context management, prompts, client initialization
  - Tests for tool execution (success, error, no executor)
  - Tests for send_message and streaming
  - All tests passing with proper mocking of Anthropic client
- Verified all acceptance criteria:
  - ✅ src/ai/claude_client.py created with async client
  - ✅ Streaming generation implemented via stream_message()
  - ✅ System prompt configured for Russian-language media concierge
  - ✅ Tool_use mechanism implemented with executor pattern

#### Learnings:
- **Anthropic SDK patterns**:
  - AsyncAnthropic for async operations
  - `messages.create()` for non-streaming requests
  - `messages.stream()` context manager for streaming
  - Content blocks can be `text` or `tool_use` types
- **Streaming event handling**:
  - `content_block_start`: Indicates new block, check type for tool_use
  - `content_block_delta`: Incremental content (text.text or input_json.partial_json)
  - `content_block_stop`: Block complete, finalize tool call if needed
  - Text deltas can be yielded immediately for progressive display
- **Tool_use conversation flow**:
  - Claude responds with tool_use blocks containing name and input
  - Must add assistant response with tool_use to conversation
  - Execute tool and add result as user message with tool_result type
  - Continue conversation until Claude responds without tool_use
  - Max iterations guard prevents infinite loops
- **Type safety with SDK**:
  - Anthropic SDK uses union types for content blocks
  - `hasattr()` checks needed for attribute access (text, partial_json)
  - Pyright warnings are acceptable for hasattr patterns
- **Testing patterns**:
  - Mock Anthropic client at module level
  - Use dataclasses for mock response objects
  - AsyncMock for async methods, MagicMock for sync

#### Architecture decisions:
- **Tool executor pattern**: Pass callable to ClaudeClient
  - Decouples client from tool implementations
  - Easy to test with mock executors
  - Allows different executors per use case
- **Context trimming**: Keep last N messages (default 20)
  - Prevents context overflow
  - Simple sliding window approach
  - Could add smarter trimming later (summarization)
- **Streaming with tools**: Non-streaming continuation
  - After tool execution, continue with non-streaming
  - Simpler implementation, tool results usually fast
  - Could optimize to stream continuation too

#### Verification:
- ✓ All 21 Claude client tests pass
- ✓ All 30 project tests pass (no regressions)
- ✓ Claude client imports work: `from src.ai.claude_client import ClaudeClient`
- ✓ System prompt contains media concierge instructions
- ✓ Preferences injection works correctly
- ✓ Linter passes: `ruff check . --fix && ruff format .`

#### Next steps:
- AI-002: Tool Definitions (define all tools for Claude API)

---

### Iteration 7 - AI-002: Tool Definitions
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Created `src/ai/tools.py` with comprehensive tool definitions module:
  - 7 tool definitions in Anthropic format with JSON schemas:
    - `rutracker_search`: Search Russian torrent tracker with quality/category filters
    - `piratebay_search`: Fallback search with min_seeds filter
    - `tmdb_search`: Movie/TV metadata from TMDB with year and language filters
    - `tmdb_credits`: Cast and crew information from TMDB
    - `kinopoisk_search`: Russian movie database with Kinopoisk ratings
    - `get_user_profile`: Get user preferences (quality, language, genres)
    - `seedbox_download`: Send magnet links to seedbox or return directly
  - All tools have detailed Russian descriptions for Claude
  - Each tool has complete JSON schema with types, enums, and required fields
- Created `ToolExecutor` class for routing tool calls:
  - Handler registration via `register_handler()` and `register_handlers()`
  - Callable interface `__call__` for ClaudeClient integration
  - Structured logging for all operations
  - Error handling with proper exception propagation
- Created helper functions:
  - `get_tool_definitions()`: Returns copy of all tools
  - `get_tool_by_name()`: Retrieve specific tool
  - `validate_tool_input()`: Validate input against JSON schema
  - `create_executor_with_stubs()`: Create executor with stub handlers for testing
- Created comprehensive test suite in `tests/test_tools.py`:
  - 34 unit tests covering all functionality
  - Tests for tool definitions structure and validity
  - Tests for individual tool schemas
  - Tests for helper functions
  - Tests for input validation
  - Tests for ToolExecutor (registration, execution, errors)
  - Tests for stub handlers and integration
- Updated `src/ai/__init__.py` to export new components
- Verified all acceptance criteria:
  - ✅ src/ai/tools.py created with all tool definitions
  - ✅ All 7 required tools defined: rutracker_search, piratebay_search, tmdb_search, tmdb_credits, kinopoisk_search, get_user_profile, seedbox_download
  - ✅ Each tool has complete JSON schema for parameters
  - ✅ Tool executor created for routing calls

#### Learnings:
- **Anthropic tool format**: Tools use `input_schema` (not `parameters`) with JSON Schema
  - Type "object" with properties, required, and optional fields
  - Enums for constrained values (quality levels, media types)
  - Descriptions in Russian for Russian-speaking assistant
- **Tool executor pattern**: Callable class for clean integration
  - `__call__` method makes executor directly usable as ClaudeClient parameter
  - Handler registration allows lazy binding of actual implementations
  - Stub handlers useful for testing tool routing before real implementations
- **Validation approach**: Simple schema validation without full JSON Schema library
  - Check required fields presence
  - Check type matching (string, integer, boolean)
  - Check enum value membership
  - Return list of errors for detailed feedback
- **Code organization**: Tools module self-contained
  - Definitions, executor, validation, and helpers in single file
  - Clear separation between definitions (data) and executor (behavior)
  - Easy to extend with new tools

#### Architecture decisions:
- **Tool definitions as module constants**: Easy access and modification
  - `ALL_TOOLS` list for iteration
  - Individual `*_TOOL` constants for specific access
  - `get_tool_definitions()` returns copy to prevent modification
- **ToolExecutor as class**: Stateful handler registry
  - Alternative: Dict-based dispatch (simpler but less flexible)
  - Alternative: Decorator-based registration (more magic, harder to test)
  - Chosen approach: Explicit registration, clear interface
- **Stub handlers for testing**: Allow testing before real implementations
  - Returns JSON with "stub" status and received input
  - `create_executor_with_stubs()` factory for quick setup

#### Verification:
- ✓ All 7 tools have valid JSON schemas (type=object, properties, required)
- ✓ Tool executor routes calls correctly to registered handlers
- ✓ All 34 tools tests pass
- ✓ All 64 project tests pass (no regressions)
- ✓ Linter passes: `ruff check . --fix && ruff format .`

#### Next steps:
- SEARCH-001: Rutracker Search Integration

---

### Iteration 8 - SEARCH-001: Rutracker Search Integration
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Verified existing `src/search/rutracker.py` implementation (735 lines)
- Comprehensive torrent search client with all required features:
  - `RutrackerClient` async context manager class
  - `search()` method with query, quality, and category filtering
  - `get_magnet_link()` for fetching magnet links from topic pages
  - `search_with_fallback()` for automatic mirror fallback
- Data models and enums:
  - `SearchResult` Pydantic model with title, size, seeds, leeches, magnet, quality
  - `VideoQuality` enum: 720p, 1080p, 4K, 2160p, HDR
  - `ContentCategory` enum: movie, tv_show, anime, documentary
- Helper functions:
  - `detect_quality()`: Regex-based quality detection from title
  - `parse_size()`: Parse human-readable sizes (GB, MB, TB) to bytes
  - `build_magnet_link()`: Generate magnet URIs with trackers
  - `extract_magnet_hash()`: Extract info hash from magnet/raw hash
- Error handling classes:
  - `RutrackerError`: Base exception
  - `RutrackerBlockedError`: Site blocked or unavailable
  - `RutrackerCaptchaError`: Captcha required
  - `RutrackerParseError`: HTML parsing failed
- Created comprehensive test suite `tests/test_rutracker.py`:
  - 43 unit tests covering all functionality
  - Tests for helper functions (quality detection, size parsing, magnet building)
  - Tests for SearchResult model
  - Tests for RutrackerClient (init, context manager, parsing, search)
  - Tests for error handling and fallback behavior
  - Sample HTML fixtures for realistic parsing tests
- Fixed linter issues:
  - Added `contextlib` import for `suppress()` usage
  - Removed unused variable `href` in get_magnet_link
  - Fixed test file issues (unused variables, nested with statements)

#### Acceptance Criteria Verification:
- ✅ src/search/rutracker.py created with complete implementation
- ✅ Search by movie/TV show title via `search()` method with `query` param
- ✅ Results parsing: title, size, seeds, leeches, magnet in `SearchResult` model
- ✅ Quality filtering (720p, 1080p, 4K, 2160p, HDR) via `quality` parameter
- ✅ Captcha handling with `RutrackerCaptchaError`
- ✅ Blocking handling with `RutrackerBlockedError` and mirror fallback

#### Learnings:
- **HTML parsing with BeautifulSoup**: Using lxml parser for speed
  - Multiple selector patterns for Rutracker's varying HTML structure
  - `select_one()` with CSS selectors for robust element finding
  - Fallback patterns (try multiple selectors) for reliability
- **Magnet link extraction**: Multiple strategies needed
  - Direct magnet link in href attribute
  - Hash in data attributes (data-hash)
  - Hash in inline scripts (regex extraction)
  - Build magnet with trackers if only hash available
- **Error categorization**: Separate exceptions for different failure modes
  - Blocked: Try different mirrors
  - Captcha: Don't retry, notify user
  - Parse error: Log but continue with partial results
- **Quality detection patterns**: Case-insensitive regex matching
  - Handle variations: "1080p", "Full HD", "FHD", "1080i"
  - Prioritize specific quality over HDR flag
- **Mirror fallback strategy**: Sequential retry with different base URLs
  - Stop on success or captcha (captcha is site-wide)
  - Continue on blocked error (region-specific)

#### Architecture:
- **Async context manager pattern**: Clean resource management
  - `__aenter__` creates httpx.AsyncClient
  - `__aexit__` closes client properly
  - Property `client` with runtime check for initialization
- **Pagination and limits**: MAX_RESULTS (20) to limit processing
- **Sorting**: Results sorted by seeds (descending) for quality
- **Caching consideration**: Magnet links fetched lazily per result

#### Verification Results:
- ✓ Module imports work correctly
- ✓ Search returns results with mocked HTML
- ✓ Results contain magnet links
- ✓ Quality filtering works (1080p, 4K)
- ✓ Blocking error message is clear
- ✓ Captcha error message is clear
- ✓ All 43 Rutracker tests pass
- ✓ All 107 project tests pass (no regressions)
- ✓ Linter passes: `ruff check . --fix && ruff format .`

#### Next steps:
- SEARCH-002: PirateBay Search Integration

---

### Iteration 9 - SEARCH-002: PirateBay Search Integration
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Created `src/search/piratebay.py` with comprehensive PirateBay search functionality (600+ lines)
- Implemented `PirateBayClient` async context manager class:
  - `search()` method with query, category, and min_seeds filtering
  - HTML parsing with multiple selector patterns for different mirrors
  - Magnet link extraction from various HTML structures
- Data models and exceptions:
  - `PirateBayResult` Pydantic model with title, size, seeds, leeches, magnet, quality
  - `PirateBayError` base exception
  - `PirateBayUnavailableError` for site unavailable/blocked scenarios
- Helper functions:
  - `detect_quality()`: Regex-based quality detection with word boundaries
  - `parse_size()`: Parse human-readable sizes (GB, MB, TB, GiB, etc.) to bytes
  - `build_magnet_link()`: Generate magnet URIs with trackers
  - `extract_magnet_link()`: Extract magnet from HTML elements
- Mirror fallback functionality:
  - `search_with_fallback()`: Tries multiple mirrors sequentially
  - 5 mirrors configured: thepiratebay.org, thepiratebay10.org, piratebay.live, thepiratebay.zone, tpb.party
  - Graceful degradation when all mirrors fail
- Category filtering support:
  - All Video (200), Movies (201), TV (205), HD Movies (207), HD TV (208)
- Created comprehensive test suite `tests/test_piratebay.py`:
  - 42 unit tests covering all functionality
  - Tests for helper functions (quality detection, size parsing, magnet building)
  - Tests for PirateBayResult model
  - Tests for PirateBayClient (init, context manager, parsing, search, filtering)
  - Tests for error handling and mirror fallback
  - Sample HTML fixtures for realistic parsing tests
- Updated `src/search/__init__.py` to export PirateBay components

#### Acceptance Criteria Verification:
- ✅ src/search/piratebay.py created with complete implementation
- ✅ Search via scraping with multiple HTML parsing patterns
- ✅ Results parsing: title, size, seeds, leeches, magnet in `PirateBayResult` model
- ✅ Category filtering (Video, Movies, TV, HD) via `category` parameter
- ✅ Fallback to mirrors via `search_with_fallback()` function

#### Learnings:
- **PirateBay HTML structure varies by mirror**: Different mirrors use different HTML layouts
  - Classic table layout: `table#searchResult tr`
  - Modern list layout: `ol#torrents li`
  - Alternative structures: `table.list tr`, `li.list-entry`, `div.detName`
  - Need multiple CSS selector patterns to handle all cases
- **Word boundaries in regex are critical**: Without `\b` boundaries, patterns like "DV" for Dolby Vision would match "DVDRip"
  - Changed from `r"DV"` to `r"\bDolby[\s._-]*Vision\b"` for accurate matching
  - Similar fixes for Full HD, Ultra HD patterns
- **BeautifulSoup type hints**: `.get()` returns `str | list[str] | None`
  - Need explicit type checking with `isinstance()` before using values
  - `Tag` type from bs4 is different from `BeautifulSoup` type
- **Cloudflare protection handling**: PirateBay mirrors often use Cloudflare
  - Detect "Cloudflare" + "challenge" in HTML
  - Raise `PirateBayUnavailableError` to trigger mirror fallback
- **Magnet link extraction strategies**:
  - Direct magnet link in anchor href
  - Hash in data attributes (data-hash)
  - Build magnet with trackers if only hash available

#### Architecture decisions:
- **Reused patterns from Rutracker**: Similar structure for maintainability
  - Async context manager pattern
  - Pydantic models for results
  - Exception hierarchy
  - Helper functions for quality detection and size parsing
- **Separate Result models**: `PirateBayResult` vs `SearchResult` from Rutracker
  - Different fields available (uploader, uploaded date for PirateBay)
  - Could unify later if needed, but separate for now
- **min_seeds filter**: Added for PirateBay (not in Rutracker)
  - Useful for filtering out dead torrents
  - Applied client-side after HTML parsing

#### Verification Results:
- ✓ Module imports work correctly
- ✓ Search returns results with mocked HTML (3 results parsed)
- ✓ Magnet links start with "magnet:?xt=urn:btih:"
- ✓ Mirror fallback works (retries on PirateBayUnavailableError)
- ✓ All 42 PirateBay tests pass
- ✓ All 149 project tests pass (no regressions)
- ✓ Linter passes: `ruff check . --fix && ruff format .`

#### Next steps:
- MEDIA-001: TMDB Integration

---

### Iteration 10 - MEDIA-001: TMDB Integration
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Created `src/media/__init__.py` with module exports
- Created `src/media/tmdb.py` with comprehensive TMDB API client (1000+ lines)
- Implemented async HTTP client using httpx:
  - `TMDBClient` class as async context manager
  - Automatic caching with configurable TTL
  - Proper error handling with typed exceptions
- Implemented search functionality:
  - `search_movie()`: Search for movies by title with optional year filter
  - `search_tv()`: Search for TV shows by title with optional year filter
  - `search_multi()`: Search both movies and TV shows together
- Implemented detailed information retrieval:
  - `get_movie()`: Get full movie details including genres, runtime, budget, revenue
  - `get_tv_show()`: Get full TV show details including seasons, episodes
- Implemented credits (cast & crew):
  - `get_movie_credits()`: Get cast and crew for movies
  - `get_tv_credits()`: Get cast and crew for TV shows
  - `get_credits()`: Universal method for either type
  - Helper methods: `get_directors()`, `get_writers()`, `get_top_cast()`
- Implemented recommendations:
  - `get_movie_recommendations()`: Get similar movies
  - `get_tv_recommendations()`: Get similar TV shows
  - `get_recommendations()`: Universal method for either type
- Implemented caching:
  - `SimpleCache` class with TTL support
  - Automatic caching of API responses
  - `clear_cache()` and `cleanup_cache()` methods
- Created Pydantic data models:
  - `Movie`, `TVShow`, `SearchResult`, `Credits`, `Person`
  - `Genre`, `ProductionCompany`
  - Helper methods for URLs, years, genre names
- Created exception hierarchy:
  - `TMDBError`: Base exception
  - `TMDBNotFoundError`: Resource not found (404)
  - `TMDBRateLimitError`: Rate limit with retry_after
  - `TMDBAuthError`: Invalid API key (401)
- Created comprehensive test suite `tests/test_tmdb.py`:
  - 55 unit tests covering all functionality
  - Tests for data models and their methods
  - Tests for cache behavior
  - Tests for client operations (search, details, credits, recommendations)
  - Tests for error handling

#### Acceptance Criteria Verification:
- ✅ src/media/tmdb.py created with async client
- ✅ Movie/TV search by title via `search_movie()`, `search_tv()`, `search_multi()`
- ✅ Detailed movie info via `get_movie()` with genres, runtime, etc.
- ✅ Credits (cast & crew) via `get_movie_credits()`, `get_tv_credits()`
- ✅ Recommendations via `get_movie_recommendations()`, `get_tv_recommendations()`
- ✅ Caching via `SimpleCache` with configurable TTL

#### Learnings:
- **TMDB API patterns**: Uses API key as query parameter, returns JSON
  - Language parameter `language=ru-RU` for Russian content
  - Pagination with page parameter
  - Different endpoints for movies vs TV shows
- **Pydantic models for API responses**: Clean data handling
  - Model validation ensures type safety
  - Helper methods on models for common operations
  - Optional fields with defaults for missing data
- **Cache design**: Simple in-memory cache sufficient for single-process bot
  - TTL-based expiration with cleanup on access
  - Cache key from endpoint + params
  - Copy on read prevents mutation issues
- **Error handling strategy**: Typed exceptions for different failure modes
  - 404 → TMDBNotFoundError (can retry with different query)
  - 429 → TMDBRateLimitError with retry_after
  - 401 → TMDBAuthError (configuration issue)
  - Other → TMDBError with status code
- **Context manager pattern**: Clean resource management for httpx client
  - `__aenter__` creates client, `__aexit__` closes it
  - Property getter raises if not in context
  - Unused exception parameters prefixed with underscore

#### Architecture decisions:
- **Separate search result model**: `SearchResult` unifies movie/TV search results
  - Contains media_type field to distinguish
  - Normalized field names (title vs name, release_date vs first_air_date)
- **Helper methods on models**: `get_poster_url()`, `get_year()`, etc.
  - Encapsulates URL building logic
  - Returns None for missing data
- **Default language**: Russian (ru-RU) for media concierge bot
  - Can be overridden per-client

#### Verification Results:
- ✓ All imports work correctly
- ✓ TMDBClient has all required methods
- ✓ SimpleCache has get/set methods
- ✓ All 55 TMDB tests pass
- ✓ All 204 project tests pass (no regressions)
- ✓ Linter passes: `ruff check . --fix && ruff format .`

#### Next steps:
- MEDIA-002: Kinopoisk Integration

---

### Iteration 11 - MEDIA-002: Kinopoisk Integration
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Verified existing `src/media/kinopoisk.py` implementation (796 lines)
- Comprehensive Kinopoisk Unofficial API client with all required features:
  - `KinopoiskClient` async context manager class
  - `search()` method for Russian and English title search
  - `get_film()` for detailed film information
  - `get_rating()` for Kinopoisk rating retrieval
  - `get_description_ru()` for Russian description
- Data models:
  - `KinopoiskFilm` with full metadata (ratings, descriptions, genres, countries)
  - `KinopoiskSearchResult` for search results
  - `KinopoiskCountry`, `KinopoiskGenre` helper models
- Graceful degradation:
  - `search_safe()`, `get_film_safe()` return None/empty on errors
  - Convenience functions: `search_kinopoisk()`, `get_kinopoisk_film()`, `get_kinopoisk_rating()`
- Error handling:
  - `KinopoiskError` base exception
  - `KinopoiskNotFoundError` (404), `KinopoiskAuthError` (401)
  - `KinopoiskRateLimitError` (402), `KinopoiskUnavailableError` (5xx, timeout)
- Caching:
  - `SimpleCache` class with TTL support
  - Automatic caching of API responses
- Comprehensive test suite: 53 tests in `tests/test_kinopoisk.py`

#### Acceptance Criteria Verification:
- ✅ src/media/kinopoisk.py created with complete implementation
- ✅ Search by Russian and English titles via `search()` method with `keyword` param
- ✅ Get Kinopoisk rating via `get_rating()` and `rating_kinopoisk` field
- ✅ Get Russian description via `get_description_ru()` and `description` field
- ✅ Graceful degradation via `*_safe()` methods and convenience functions

#### Learnings:
- **Kinopoisk API authentication**: Uses X-API-KEY header (not query param like TMDB)
  - API token from kinopoiskapiunofficial.tech
  - Rate limit indicated by 402 status code (not 429)
- **API versioning**: Different endpoints for different API versions
  - v2.2: `/v2.2/films` for search with type filter
  - v2.1: `/v2.1/films/search-by-keyword` for keyword search
  - Field names differ between versions (filmId vs kinopoiskId)
- **Pydantic aliases for snake_case**: API returns camelCase
  - Use `Field(..., alias="camelCase")` for mapping
  - Set `model_config = {"populate_by_name": True}` for both name styles
- **Graceful degradation pattern**: Critical for bot reliability
  - Regular methods raise exceptions for caller control
  - `*_safe()` methods return None/empty for fire-and-forget calls
  - Convenience functions use safe methods by default
- **Code reuse**: Similar patterns to TMDB client
  - Async context manager, SimpleCache, exception hierarchy
  - Enables consistent API across media sources

#### Architecture decisions:
- **Dual search endpoints**: v2.2 for type filtering, v2.1 for simpler keyword search
- **Response mapping**: Normalize v2.1 response to v2.2 model format
- **Helper methods**: `get_title()`, `get_english_title()`, `get_genre_names()`
- **is_tv_series()**: Check both `serial` flag and `type` field

#### Verification Results:
- ✓ Search "Брат" returns correct film
- ✓ Kinopoisk rating present (8.1)
- ✓ Russian description present
- ✓ Bot continues working on API error (graceful degradation)
- ✓ All 53 Kinopoisk tests pass
- ✓ All 257 project tests pass (no regressions)
- ✓ Linter passes: `ruff check . --fix && ruff format .`

#### Next steps:
- SEEDBOX-001: Seedbox Integration

---

### Iteration 12 - SEEDBOX-001: Seedbox Integration
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Created `src/seedbox/__init__.py` with module exports
- Created `src/seedbox/client.py` with comprehensive seedbox client (900+ lines)
- Implemented support for multiple torrent client APIs:
  - `TransmissionClient`: Transmission RPC API with CSRF token handling
  - `QBittorrentClient`: qBittorrent Web API with session cookies
  - `DelugeClient`: Deluge JSON-RPC API with daemon connection
- Data models and exceptions:
  - `TorrentInfo`: Torrent metadata (hash, name, status, progress, size, etc.)
  - `TorrentStatus` enum: downloading, seeding, paused, checking, queued, error, unknown
  - `SeedboxType` enum: transmission, qbittorrent, deluge
  - `SeedboxError`, `SeedboxAuthError`, `SeedboxConnectionError`, `SeedboxTorrentError`
- Implemented core functionality:
  - `add_magnet()`: Send magnet links to seedbox
  - `get_torrent_status()`: Get torrent progress and status
  - `list_torrents()`: List all torrents on seedbox
- Helper functions:
  - `detect_seedbox_type()`: Auto-detect client type from URL
  - `create_seedbox_client()`: Factory for creating appropriate client
  - `extract_magnet_hash()`: Extract info hash from magnet links
  - `is_seedbox_configured()`: Check if seedbox credentials available
- Convenience functions with graceful degradation:
  - `send_magnet_to_seedbox()`: Send magnet or return link if seedbox not configured
  - `get_torrent_status()`: Get status or return error if seedbox not configured
- Created comprehensive test suite `tests/test_seedbox.py`:
  - 51 unit tests covering all functionality
  - Tests for all three client types
  - Tests for helper functions and convenience functions
  - Tests for error handling and graceful degradation

#### Acceptance Criteria Verification:
- ✅ src/seedbox/client.py created with complete implementation
- ✅ Support Transmission/qBittorrent/Deluge API (all three clients implemented)
- ✅ Send magnet link for download via `add_magnet()` method
- ✅ Get download status via `get_torrent_status()` method
- ✅ Graceful degradation when seedbox not configured (returns magnet link)

#### Learnings:
- **Transmission RPC authentication**: Uses CSRF token (X-Transmission-Session-Id)
  - Initial request returns 409 with session ID in header
  - Must include session ID in all subsequent requests
  - Token can expire, need to handle 409 and refresh
- **qBittorrent authentication**: Uses session cookies
  - POST to /api/v2/auth/login with form data
  - Response "Ok." indicates success
  - Cookie SID must be included in subsequent requests
  - 403 indicates session expiry, need to re-authenticate
- **Deluge authentication**: Uses JSON-RPC with password
  - Password auth (not username/password)
  - May need to connect to daemon after web login
  - Check web.connected and use web.connect if needed
- **Magnet hash formats**: Can be hex (40 chars) or base32 (32 chars)
  - Base32 hashes need to be decoded to hex
  - Use `base64.b32decode()` for conversion
  - Only convert if 32 chars AND not all hex characters
- **Exception chaining**: Python best practice (B904)
  - Use `raise NewError(...) from original_error`
  - Preserves stack trace for debugging
  - Required by ruff linter
- **Status mapping**: Each client has different status values
  - Transmission: numeric codes (0-6)
  - qBittorrent: string states (downloading, uploading, pausedDL, etc.)
  - Deluge: capitalized strings (Downloading, Seeding, etc.)

#### Architecture decisions:
- **Abstract base class pattern**: `SeedboxClient` defines interface
  - Concrete implementations for each client type
  - Easy to add new client types
  - Factory function creates appropriate type
- **Auto-detection strategy**: Based on URL patterns
  - Port numbers (9091, 8080, 8112) indicate client type
  - Client name in URL also detected
  - Default to Transmission (most common)
- **Graceful degradation**: Essential for optional feature
  - Check `is_seedbox_configured()` before operations
  - Return magnet link directly if not configured
  - User can still use the link manually

#### Verification Results:
- ✓ Code compiles without errors
- ✓ Seedbox not configured → magnet link returned to user
- ✓ All 51 seedbox tests pass
- ✓ All 308 project tests pass (no regressions)
- ✓ Linter passes: `ruff check . --fix && ruff format .`

#### Next steps:
- USER-001: User Profile Storage

---

### Iteration 13 - USER-001: User Profile Storage
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Created `src/user/__init__.py` with module exports
- Created `src/user/storage.py` with comprehensive SQLite-based user storage (1200+ lines)
- Implemented database schema with 4 main tables:
  - `users`: User profiles with telegram_id, username, names, language
  - `credentials`: Encrypted OAuth tokens and sensitive credentials
  - `preferences`: User preferences (quality, language, genres, notifications)
  - `watched`: Watch history with TMDB/Kinopoisk IDs
- Implemented encryption for OAuth tokens:
  - `EncryptionHelper` class using Fernet symmetric encryption
  - Base64 encoding for storage in SQLite TEXT columns
  - Automatic decryption on retrieval with expiration checking
- Implemented full CRUD operations:
  - `create_user()`, `get_user()`, `get_user_by_telegram_id()`
  - `get_or_create_user()` with automatic info update
  - `update_user()`, `delete_user()`, `list_users()` with pagination
  - Credential storage/retrieval with encryption
  - Preferences management with JSON arrays for genres
  - Watch history tracking with is_watched() check
- Implemented database migrations:
  - `MigrationManager` class with version tracking
  - 5 migrations for all tables plus _migrations tracking table
  - Idempotent migrations (safe to run multiple times)
  - Automatic migration application on connect
- Created Pydantic data models:
  - `User`, `Credential`, `Preference`, `WatchedItem`
  - `CredentialType` enum for token types
  - Helper properties like `display_name`
- Created comprehensive test suite `tests/test_user_storage.py`:
  - 49 unit tests covering all functionality
  - Tests for encryption, migrations, CRUD, credentials
  - Tests for context manager and convenience functions

#### Acceptance Criteria Verification:
- ✅ src/user/storage.py created with SQLite via aiosqlite
- ✅ Schema: users, credentials, preferences, watched tables
- ✅ OAuth token encryption using Fernet (cryptography library)
- ✅ CRUD operations for all user data
- ✅ Database migrations with version tracking

#### Learnings:
- **Fernet encryption**: Symmetric encryption ideal for credential storage
  - Key must be 32 bytes URL-safe base64 encoded
  - Includes timestamp in ciphertext (same plaintext → different ciphertext)
  - Base64 encoding allows storage in TEXT columns
- **aiosqlite patterns**: Async wrapper around sqlite3
  - Row factory for dict-like access (row["column"])
  - Foreign keys require explicit `PRAGMA foreign_keys = ON`
  - `executescript()` for multi-statement SQL (CREATE TABLE + INDEX)
- **Migration strategy**: Sequential version-based migrations
  - `_migrations` table tracks applied versions
  - Check table existence before version query
  - Idempotent migrations using `IF NOT EXISTS`
- **datetime.utcnow() deprecated**: Use `datetime.now(UTC)` instead
  - Import `from datetime import UTC, datetime`
  - Pyright flags utcnow() as deprecated
- **Type safety with async context managers**:
  - Use underscore prefix for unused exc_type, exc_val, exc_tb
  - Return type tuple[User, bool] requires non-None User
- **JSON storage for arrays**: Store genre lists as JSON strings
  - `json.dumps()` for writing, `json.loads()` for reading
  - Handle None/empty string → default to "[]"

#### Architecture decisions:
- **Single file approach**: All storage logic in one file
  - Models, encryption, migrations, CRUD in storage.py
  - Easy to understand and test
  - Could split later if needed
- **Async context manager**: Clean resource management
  - `async with UserStorage(...) as storage`
  - Automatic migration on connect
  - Automatic close on exit
- **Credential encryption required**: No plaintext storage allowed
  - `store_credential()` raises if no encryption key
  - Prevents accidental insecure usage
- **Default preferences on user create**: Simpler API
  - Creating user also creates preferences row
  - Avoids null checks when getting preferences
- **ON CONFLICT for upsert**: Credential updates via INSERT OR REPLACE
  - Cleaner than separate INSERT/UPDATE logic
  - Atomic operation

#### Verification Results:
- ✓ User creation works correctly
- ✓ Tokens stored encrypted (verified in DB - not plaintext)
- ✓ Token decryption returns original value
- ✓ All migrations applied (5 migrations)
- ✓ All tables created (users, credentials, preferences, watched, _migrations)
- ✓ All 49 user storage tests pass
- ✓ All 357 project tests pass (no regressions)
- ✓ Linter passes: `ruff check . --fix && ruff format .`

#### Next steps:
- ONBOARD-001: User Onboarding Flow

---

### Iteration 14 - ONBOARD-001: User Onboarding Flow
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Created `src/bot/onboarding.py` with comprehensive onboarding module (950+ lines)
- Implemented welcome message with inline keyboard buttons:
  - "🎬 Настроить предпочтения" to start setup
  - "⏭ Пропустить настройку" to use defaults
- Implemented multi-step preference setup flow:
  - Video quality selection (720p, 1080p, 4K)
  - Audio language selection (Русский, English, Оригинал)
  - Genre preference selection with multi-select (6 genres)
- Implemented settings command (/settings) with inline buttons:
  - View and change current quality
  - View and change current audio language
  - View and change preferred genres
- Updated `src/bot/main.py` to register new handlers:
  - CommandHandler for /start (onboarding_start_handler)
  - CommandHandler for /settings (settings_handler)
  - CallbackQueryHandler for onboard_* callbacks
  - CallbackQueryHandler for settings_* callbacks
- User profile creation via UserStorage integration:
  - Automatic user creation on /start via get_or_create_user()
  - Preferences saved to SQLite database
  - Graceful error handling if storage fails
- Created comprehensive test suite `tests/test_onboarding.py`:
  - 23 unit tests covering all functionality
  - Tests for keyboard generation
  - Tests for message templates
  - Tests for handlers (/start, /settings)
  - Tests for callback handlers (quality, audio, genre selection)
  - Integration test for full onboarding flow

#### Acceptance Criteria Verification:
- ✅ Приветственное сообщение с inline кнопками (welcome message with inline buttons)
- ✅ Создание профиля пользователя при /start (user profile created on /start)
- ✅ Настройка предпочтений (качество видео, язык аудио) (preferences setup)
- ✅ Inline кнопки для быстрой настройки (inline buttons for quick setup)

#### Learnings:
- **Inline keyboard callbacks**: Use pattern-based CallbackQueryHandler
  - `pattern="^onboard_"` for onboarding callbacks
  - `pattern="^settings_"` for settings callbacks
  - Prevents collision between different callback groups
- **Multi-step wizard pattern**: Use context.user_data to track state
  - Store selected values between callback calls
  - Clear on completion or timeout
  - Need to handle None user_data case (type safety)
- **Telegram edit_message_text**: Replace message content inline
  - Better UX than sending new messages
  - Works with or without keyboard
  - Can use edit_message_reply_markup for keyboard-only updates
- **Type safety with context.user_data**: Can be None
  - Always check `if context.user_data is not None` before subscript
  - Use conditional expression: `context.user_data.get(...) if context.user_data else default`
- **Pyright warnings for conditional access**: False positives on ternary
  - `user_data.get(...) if user_data else []` still shows warning
  - Acceptable to ignore these specific warnings
- **Unused parameters**: Use underscore prefix (_context) for callbacks
  - Some handlers don't need context but must match signature
  - Ruff/pyright accept underscore-prefixed unused params

#### Architecture decisions:
- **Separate onboarding module**: Clean separation from handlers.py
  - All onboarding logic in single file
  - Easy to maintain and extend
  - Clear export in __init__.py
- **Callback data naming convention**: Prefix-based routing
  - `onboard_*` for onboarding flow
  - `settings_*` for settings menu
  - Clear, non-colliding namespaces
- **Settings keyboard dynamic**: Show current values
  - `get_settings_keyboard(current_quality, current_audio)`
  - User sees what they have selected
  - One-tap to change any setting
- **Genre multi-select**: Toggle pattern with checkmarks
  - ✅ prefix shows selected state
  - Store in list, toggle on click
  - Save only on explicit "Done" button

#### Verification Results:
- ✓ /start shows welcome message with buttons
- ✓ Profile created in database (get_or_create_user called)
- ✓ Preferences saved via update_preferences
- ✓ All 23 onboarding tests pass
- ✓ All 380 project tests pass (no regressions)
- ✓ Linter passes: `ruff check . --fix && ruff format .`

#### Next steps:
- CONV-001: Natural Language Search

---

### Iteration 15 - CONV-002: Context-Aware Responses
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Enhanced `src/ai/prompts.py` with improved user preference handling:
  - Updated `get_system_prompt()` to use correct field names (quality, audio_language, genres)
  - Added backward compatibility for old field names (preferred_quality, preferred_language, favorite_genres)
  - Added explicit instructions for Claude to use quality filter in torrent searches
  - Added instructions for Claude to consider genre preferences in recommendations
- Updated `src/ai/claude_client.py`:
  - Added `telegram_user_id` field to `ConversationContext` dataclass
  - Enables tool calls to access user ID when needed
- Updated `src/bot/conversation.py`:
  - Set `telegram_user_id` on conversation context from Telegram user
  - Added debug logging when user preferences are loaded
  - Preferences (quality, audio_language, genres) loaded from database on each message
- Created comprehensive test suite for context-aware functionality:
  - Tests for conversation context preserving user preferences
  - Tests for telegram_user_id storage
  - Tests for message preservation within sessions
  - Tests for system prompt generation with all preference types
  - Tests for backward compatibility with old field names

#### Acceptance Criteria Verification:
- ✅ Claude помнит предпочтения пользователя из профиля (user preferences loaded into context)
- ✅ Рекомендации учитывают жанровые предпочтения (genre instructions added to system prompt)
- ✅ Контекст диалога сохраняется в рамках сессии (ConversationContext preserves messages)
- ✅ Качество видео подбирается по предпочтениям (quality filter instructions added to prompt)

#### Learnings:
- **Field naming consistency**: Important to match field names between storage layer and prompts
  - Storage: video_quality, audio_language, preferred_genres
  - Conversation context: quality, audio_language, genres
  - Added compatibility layer in get_system_prompt() for both naming conventions
- **System prompt engineering for tools**: Adding explicit instructions in the system prompt
  - "ВСЕГДА используй фильтр quality=\"X\"" ensures Claude passes quality to search tools
  - "Учитывай эти жанры при рекомендациях" guides recommendation behavior
- **Context persistence pattern**: ConversationContext stored in module-level dict by user_id
  - Same context object returned for same user_id
  - Messages accumulate within session
  - clear() only clears messages, not preferences (by design)
- **User ID propagation**: telegram_user_id in context enables tools to access user profile
  - get_user_profile tool can use this to fetch fresh preferences
  - Enables Claude to query profile when needed

#### Architecture decisions:
- **Per-user conversation context**: Dict[user_id, ConversationContext]
  - Simple and effective for single-instance bot
  - Could extend to Redis for multi-instance deployment
- **Preferences in system prompt**: Most reliable way to influence Claude behavior
  - Alternative: Pass as tool context (less reliable)
  - Alternative: First message system info (clutters conversation)
- **Debug logging for preferences**: Helps troubleshoot context issues
  - Logs quality, audio_language, genres on each message
  - Uses debug level to avoid log noise in production

#### Verification Results:
- ✓ System prompt includes quality preference and filter instruction
- ✓ System prompt includes audio language preference
- ✓ System prompt includes genre preferences with recommendation instruction
- ✓ ConversationContext stores user_preferences and telegram_user_id
- ✓ Context preserves messages within session
- ✓ All 35 conversation tests pass
- ✓ All 415 project tests pass (no regressions)
- ✓ Linter passes: `ruff check . --fix && ruff format .`

#### Next steps:
- DEPLOY-001: Koyeb Deployment

---

### Iteration 16 - DEPLOY-001: Koyeb Deployment
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Created HTTP health check endpoint on dedicated port (8080):
  - Implemented `handle_health_request()` minimal HTTP server using asyncio streams
  - Returns JSON `{"status": "healthy", "service": "media-concierge-bot", "ready": true}`
  - Returns 503 during startup, 200 when bot is ready
  - Runs on separate port from webhook (8080 vs 8000) for cleaner separation
- Updated `src/bot/main.py`:
  - Added `_bot_healthy` global flag to track readiness
  - Added `start_health_server()` function to create asyncio server
  - Updated `run_webhook()` to start health server and mark healthy after initialization
  - Updated `run_polling()` to also start health server for local testing
- Created `koyeb.yaml` deployment configuration:
  - Docker deployment from Dockerfile
  - nano instance type (free tier eligible)
  - Frankfurt (fra) region
  - Health check on port 8080 path /health
  - Environment variables referencing Koyeb secrets
  - Rolling deployment strategy
- Updated Dockerfile:
  - Added `HEALTH_PORT=8080` environment variable
  - Updated HEALTHCHECK to use health port
  - Exposed both ports 8000 (webhook) and 8080 (health)
  - Increased start-period to 60s for slower startups
- Updated `src/config.py`:
  - Added `health_port` setting with default 8080
- Updated `.env.example`:
  - Added HEALTH_PORT, WEBHOOK_PATH, ENVIRONMENT variables
  - Better documentation for deployment settings

#### Acceptance Criteria Verification:
- ✅ Dockerfile оптимизирован для продакшена (multi-stage build, non-root user)
- ✅ koyeb.yaml с конфигурацией сервиса (ports, health checks, env vars, scaling)
- ✅ Health check endpoint /health (implemented on port 8080)
- ✅ Webhook URL настраивается автоматически (WEBHOOK_URL env var)
- ✅ Переменные окружения из Koyeb secrets (all secrets referenced in koyeb.yaml)

#### Learnings:
- **Separate health port approach**: Using a dedicated port for health checks
  - Avoids conflict with webhook endpoint
  - Cleaner separation of concerns
  - Koyeb can monitor health without interfering with Telegram updates
- **asyncio TCP server for HTTP**: Simple approach for minimal health endpoint
  - No external dependencies needed
  - `asyncio.start_server()` creates TCP server
  - Manual HTTP response formatting (simple but effective)
  - Handles multiple concurrent requests
- **Global health flag pattern**: `_bot_healthy` tracks readiness
  - False during startup
  - True after webhook is set and bot is initialized
  - Reset to False during shutdown
  - 503 returned when not ready, 200 when healthy
- **Koyeb deployment configuration**:
  - Multiple ports can be configured in koyeb.yaml
  - Secrets are referenced by name (not value)
  - Health checks have grace period for startup
  - Rolling deployment for zero-downtime updates
- **contextlib.suppress()**: Cleaner than try/except/pass
  - Used for writer.wait_closed() which may fail
  - Ruff prefers this over empty except blocks

#### Architecture decisions:
- **Two-port architecture**: Webhook (8000) + Health (8080)
  - Alternative: Single port with path routing (more complex)
  - Alternative: Health on same webhook server (requires starlette)
  - Chosen approach: Simple, no dependencies, clear separation
- **asyncio TCP server**: Pure Python, no framework
  - Alternative: aiohttp (adds dependency)
  - Alternative: starlette (python-telegram-bot's default, complex setup)
  - Chosen approach: Zero dependencies, small footprint
- **Health response format**: JSON with status and readiness
  - Includes `ready` field for Koyeb probes
  - Includes `service` field for identification
  - Standard HTTP status codes (200/503)

#### Verification Results:
- ✓ Dockerfile syntax valid (multi-stage build, two ports exposed)
- ✓ koyeb.yaml created with complete configuration
- ✓ Health server functions import correctly
- ✓ /health would return 200 OK when bot is healthy
- ✓ All 415 project tests pass (no regressions)
- ✓ Linter passes: `ruff check . --fix && ruff format .`

#### Notes on Docker verification:
- Docker build verification requires Docker daemon
- Estimated image size: ~300-400MB based on similar Python images
- Multi-stage build strips development dependencies
- Final image uses python:3.11-slim base (~150MB)

#### Next steps:
- TEST-001: Unit Tests (coverage > 70%)

---

### Iteration 17 - TEST-001: Unit Tests
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Verified existing comprehensive test suite covering all critical components
- Confirmed test coverage meets requirements (75.00% overall)
- Test file inventory:
  - `tests/test_rutracker.py`: 43+ tests for Rutracker parser
  - `tests/test_piratebay.py`: 42+ tests for PirateBay parser
  - `tests/test_tmdb.py`: 55+ tests for TMDB client
  - `tests/test_user_storage.py`: 49+ tests for user storage
  - `tests/test_tools.py`: 34+ tests for tool executor
  - Additional test files: test_claude_client.py, test_conversation.py, test_kinopoisk.py, test_onboarding.py, test_seedbox.py, test_streaming.py
- Total: 415 tests passing

#### Acceptance Criteria Verification:
- ✅ Тесты для парсеров Rutracker/PirateBay (test_rutracker.py, test_piratebay.py)
- ✅ Тесты для TMDB клиента (test_tmdb.py with 55+ tests)
- ✅ Тесты для user storage (test_user_storage.py with 49+ tests)
- ✅ Тесты для tool executor (test_tools.py with 34+ tests)
- ✅ Coverage > 70% (75.00% achieved)

#### Coverage breakdown by module:
- src/ai/: 68-100% (claude_client 68.60%, prompts 100%, tools 96.47%)
- src/bot/: 18-78% (handlers 22.58%, main 18.32%, conversation 70.83%, onboarding 61.52%, streaming 78.07%)
- src/media/: 90-100% (tmdb 90.09%, kinopoisk 93.82%)
- src/search/: 72-83% (piratebay 72.76%, rutracker 83.53%)
- src/seedbox/: 58.49%
- src/user/: 94.24%
- src/config.py: 76.36%
- src/logger.py: 93.02%

#### Learnings:
- **Test coverage was already achieved**: Previous iterations built comprehensive test suites
  - Each feature implementation included unit tests
  - Tests for parsers, clients, storage, and tools all existed
- **Coverage calculation**: pytest-cov provides accurate line-by-line coverage
  - TOTAL line shows overall percentage
  - Missing column shows untested line numbers
- **Bot modules have lower coverage**: handlers.py (22.58%) and main.py (18.32%)
  - These modules are harder to test without running actual Telegram bot
  - Integration tests would require Telegram API mocking
  - Core business logic has high coverage, which is more important
- **Critical components well covered**:
  - Parsers (Rutracker/PirateBay): 72-83%
  - Media clients (TMDB/Kinopoisk): 90-94%
  - User storage: 94%
  - Tool definitions and executor: 96%

#### Architecture notes:
- **Test organization**: One test file per module, matching src/ structure
- **Async testing**: pytest-asyncio handles async test functions
- **Mocking strategy**: unittest.mock.AsyncMock for async functions, MagicMock for sync
- **Fixtures**: Sample HTML and JSON responses for realistic parsing tests

#### Verification Results:
- ✓ pytest проходит без ошибок (415 passed in 1.61s)
- ✓ Coverage report показывает > 70% (75.00%)
- ✓ All acceptance criteria met

#### Next steps:
- DOC-001: Documentation (final task)

---

### Iteration 18 - DOC-001: Documentation
**Date:** 2026-01-18
**Status:** ✅ COMPLETED

#### What was done:
- Created `docs/ARCHITECTURE.md` with comprehensive system architecture documentation:
  - ASCII diagrams showing system components and data flow
  - Module overview with file structure for each module
  - Database schema documentation
  - Tool flow diagram for Claude API integration
  - Deployment architecture diagram
  - Security and performance considerations
- Created `docs/API.md` with detailed tool documentation:
  - Overview table of all 7 Claude tools
  - Full documentation for each tool with parameters, examples, and responses
  - ToolExecutor usage guide
  - Integration example with ClaudeClient
  - Error handling documentation
  - Rate limits reference
- Created `docs/DEPLOYMENT.md` with Koyeb deployment guide:
  - Prerequisites and API key sources
  - Step-by-step deployment instructions
  - Koyeb secrets configuration
  - Environment variables reference
  - koyeb.yaml reference
  - Troubleshooting section
  - Monitoring and cost estimation
- Updated `README.md`:
  - Added Documentation section with links to all docs
  - Improved Deployment to Koyeb instructions
- Verified all public functions have docstrings (0 missing)

#### Acceptance Criteria Verification:
- ✅ README с quick start (already present, enhanced with docs links)
- ✅ docs/ARCHITECTURE.md с диаграммами (created with ASCII diagrams)
- ✅ docs/API.md с описанием tools (all 7 tools documented)
- ✅ docs/DEPLOYMENT.md с инструкцией для Koyeb (comprehensive guide)
- ✅ Inline документация в коде (docstrings) (all public functions have docstrings)

#### Learnings:
- **ASCII diagrams work well in markdown**: Clear visual representation without images
  - Use monospace formatting for alignment
  - Box drawing characters (+ | -) for simple diagrams
- **Documentation structure**: Three docs cover different audiences
  - ARCHITECTURE.md: For developers understanding the system
  - API.md: For integrating with Claude tools
  - DEPLOYMENT.md: For deploying to production
- **Tool documentation pattern**: Consistent format helps usability
  - Description, parameters table, example request, example response
  - Error response examples important for debugging
- **Docstring verification with AST**: Python's ast module can check docstrings programmatically
  - Walk the AST tree and check FunctionDef/AsyncFunctionDef nodes
  - Skip private methods (starting with _)
  - Check first body element is Expr with Constant value

#### Verification Results:
- ✓ README contains quick start instructions
- ✓ docs/ARCHITECTURE.md created with diagrams (9KB)
- ✓ docs/API.md created with tool descriptions (9KB)
- ✓ docs/DEPLOYMENT.md created with Koyeb instructions (8KB)
- ✓ All public functions have docstrings (0 missing)
- ✓ All 415 tests pass (no regressions)
- ✓ Linter passes: `ruff check . --fix && ruff format .`

---

## PROJECT COMPLETE

All 19 user stories have been implemented and verified:
- INFRA-001: Project Structure Setup ✅
- INFRA-002: Configuration Management ✅
- INFRA-003: Logging Setup ✅
- BOT-001: Basic Telegram Bot ✅
- BOT-002: Message Streaming ✅
- AI-001: Claude API Integration ✅
- AI-002: Tool Definitions ✅
- SEARCH-001: Rutracker Search ✅
- SEARCH-002: PirateBay Search ✅
- MEDIA-001: TMDB Integration ✅
- MEDIA-002: Kinopoisk Integration ✅
- SEEDBOX-001: Seedbox Integration ✅
- USER-001: User Profile Storage ✅
- ONBOARD-001: User Onboarding Flow ✅
- CONV-001: Natural Language Search ✅
- CONV-002: Context-Aware Responses ✅
- DEPLOY-001: Koyeb Deployment ✅
- TEST-001: Unit Tests (75% coverage) ✅
- DOC-001: Documentation ✅

